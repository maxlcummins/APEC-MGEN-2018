{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Australian Avian Pathogenic *E. coli* - Collection 1\n",
    "This is a Jupyter notebook documenting the workflow for the analysis of a collection of *E. coli* provided by the University of Melbourne. The collection consists of *E. coli* that originate from poultry operations around Australia.\n",
    "\n",
    "This will follow the genotyping and phylogrouping of the samples, through detection of genes associated with virulence, antimicrobial resistance, plasmid carriage, phylogroup, e-serotype and multi-locus sequence type with [ARIBA](https://github.com/sanger-pathogens/ariba/wiki) and its subsequent processing with [ARIBAlord](https://github.com/maxlcummins/ARIBAlord).\n",
    "\n",
    "Subsequent notebooks will follow genome assembly and generation of phylogenetic trees using snippy and phylosift.\n",
    "\n",
    "This workflow assumes you have miniconda3 installed **(Note the 3! Let's leave Python 2 in the past...)**. If you do not, open a new terminal window and enter the following commands and follow any installation prompts:\n",
    "\n",
    "```wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh```\n",
    "\n",
    "```bash Miniconda3-latest-Linux-x86_64.sh```\n",
    "\n",
    "Then switch back to this workbook and continue as normal. If you come across any issues you can find installation instructions for miniconda [here](https://conda.io/docs/user-guide/install/index.html)\n",
    "\n",
    "If you would like to work with the reads for these samples (and you have access to the UTS HPC and the s1 directory) then I suggest you copy them from their path to your home directory as follows - this way you should be able to use all of the commands below as is without changing any paths! \n",
    "\n",
    "**Note: This may take some time, the files are large...**\n",
    "\n",
    "```mkdir ~/BigChook_Reads```\n",
    "\n",
    "```cp -r ~/../s1/Max_Cummins/BigChook_Reads/reads ~/BigChook_Reads/reads```\n",
    "\n",
    "**Note to external users/reviewers: PBS job submission scripts will likely require modifications if they are to work!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIBA - genotyping and phylogenetic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -y -n ariba -c bioconda ariba\n",
    "!conda create -y -n aribalord\n",
    "!conda install -n aribalord git pandas regex -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source activate ariba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sourcing ARIBA databases\n",
    "We now download our ARIBA databases from the [Centre for Genomic Epidemiology](https://cge.cbs.dtu.dk/services/cge/).\n",
    "Note that we also create an info file for each database. It is important that we know which databases versions are used for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `/shared/homes/11025234/ARIBA_databases': File exists\n",
      "ariba db directory prepared. You can use it like this:\n",
      "ariba run /shared/homes/11025234/ARIBA_databases/get_mlst/ref_db reads_1.fq reads_2.fq output_directory\n",
      "--2018-11-16 13:39:19--  https://raw.githubusercontent.com/maxlcummins/E_coli_customDB/master/EC_customDB.fasta\n",
      "Resolving raw.githubusercontent.com... 151.101.28.133\n",
      "Connecting to raw.githubusercontent.com|151.101.28.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 373498 (365K) [text/plain]\n",
      "Saving to: `/shared/homes/11025234/ARIBA_databases/EC_customDB.fasta'\n",
      "\n",
      "100%[======================================>] 373,498     --.-K/s   in 0.008s  \n",
      "\n",
      "2018-11-16 13:39:19 (43.4 MB/s) - `/shared/homes/11025234/ARIBA_databases/EC_customDB.fasta' saved [373498/373498]\n",
      "\n",
      "--2018-11-16 13:39:20--  https://raw.githubusercontent.com/maxlcummins/E_coli_customDB/master/E_coli_phylogroup.fasta\n",
      "Resolving raw.githubusercontent.com... 151.101.28.133\n",
      "Connecting to raw.githubusercontent.com|151.101.28.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4867 (4.8K) [text/plain]\n",
      "Saving to: `/shared/homes/11025234/ARIBA_databases/E_coli_phylogroup.fasta'\n",
      "\n",
      "100%[======================================>] 4,867       --.-K/s   in 0s      \n",
      "\n",
      "2018-11-16 13:39:20 (66.2 MB/s) - `/shared/homes/11025234/ARIBA_databases/E_coli_phylogroup.fasta' saved [4867/4867]\n",
      "\n",
      "--2018-11-16 13:39:20--  https://raw.githubusercontent.com/katholt/srst2/master/data/EcOH.fasta\n",
      "Resolving raw.githubusercontent.com... 151.101.28.133\n",
      "Connecting to raw.githubusercontent.com|151.101.28.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 801017 (782K) [text/plain]\n",
      "Saving to: `/shared/homes/11025234/ARIBA_databases/EcOH.fasta'\n",
      "\n",
      "100%[======================================>] 801,017     --.-K/s   in 0.02s   \n",
      "\n",
      "2018-11-16 13:39:20 (47.5 MB/s) - `/shared/homes/11025234/ARIBA_databases/EcOH.fasta' saved [801017/801017]\n",
      "\n",
      "mkdir: cannot create directory `database_info': File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir ~/ARIBA_databases\n",
    "!cd ~/ARIBA_databases\n",
    "!ariba getref plasmidfinder ~/ARIBA_databases/plasmidfinder > plasmidfinder_version.info\n",
    "!ariba getref resfinder ~/ARIBA_databases/resfinder > resfinder_version.info\n",
    "!ariba getref virulencefinder ~/ARIBA_databases/virulencefinder > virulencefinder_version.info\n",
    "!ariba pubmlstget \"Escherichia coli#1\" ~/ARIBA_databases/get_mlst\n",
    "!wget https://raw.githubusercontent.com/maxlcummins/E_coli_customDB/master/EC_customDB.fasta -P ~/ARIBA_databases\n",
    "!wget https://raw.githubusercontent.com/maxlcummins/E_coli_customDB/master/E_coli_phylogroup.fasta -P ~/ARIBA_databases\n",
    "!wget https://raw.githubusercontent.com/maxlcummins/E_coli_customDB/master/O_H_types.fsa -P ~/ARIBA_databases\n",
    "    \n",
    "!echo \"MLST database accessed on:\" > MLST_version.info; date >> MLST_version.info\n",
    "!echo \"EC_customDB database accessed on:\" > EC_customDB.info; date >> EC_customDB.info\n",
    "!echo \"E_coli_phylogroup database accessed on:\" > E_coli_phylogroup.info; date >> E_coli_phylogroup.info\n",
    "!echo \"serofinder database accessed on:\" > serofinder_EcOH.info; date >> serofinder_EcOH.info\n",
    "!mkdir ~/ARIBA_databases/database_info; mv ~/ARIBA_databases/*.info ~/ARIBA_databasesdatabase_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing ARIBA databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/shared/homes/11025234/miniconda3/bin/ariba\", line 292, in <module>\n",
      "    args.func(args)\n",
      "  File \"/shared/homes/11025234/miniconda3/lib/python3.6/site-packages/ariba/tasks/prepareref.py\", line 31, in run\n",
      "    preparer.run(options.outdir)\n",
      "  File \"/shared/homes/11025234/miniconda3/lib/python3.6/site-packages/ariba/ref_preparer.py\", line 146, in run\n",
      "    raise Error('Error! Output directory ' + outdir + ' already exists. Cannot continue')\n",
      "ariba.ref_preparer.Error: Error! Output directory /shared/homes/11025234/ARIBA_databases/plasmidfinder.prepareref already exists. Cannot continue\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/homes/11025234/miniconda3/bin/ariba\", line 292, in <module>\n",
      "    args.func(args)\n",
      "  File \"/shared/homes/11025234/miniconda3/lib/python3.6/site-packages/ariba/tasks/prepareref.py\", line 31, in run\n",
      "    preparer.run(options.outdir)\n",
      "  File \"/shared/homes/11025234/miniconda3/lib/python3.6/site-packages/ariba/ref_preparer.py\", line 146, in run\n",
      "    raise Error('Error! Output directory ' + outdir + ' already exists. Cannot continue')\n",
      "ariba.ref_preparer.Error: Error! Output directory /shared/homes/11025234/ARIBA_databases/resfinder.prepareref already exists. Cannot continue\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/homes/11025234/miniconda3/bin/ariba\", line 292, in <module>\n",
      "    args.func(args)\n",
      "  File \"/shared/homes/11025234/miniconda3/lib/python3.6/site-packages/ariba/tasks/prepareref.py\", line 31, in run\n",
      "    preparer.run(options.outdir)\n",
      "  File \"/shared/homes/11025234/miniconda3/lib/python3.6/site-packages/ariba/ref_preparer.py\", line 146, in run\n",
      "    raise Error('Error! Output directory ' + outdir + ' already exists. Cannot continue')\n",
      "ariba.ref_preparer.Error: Error! Output directory /shared/homes/11025234/ARIBA_databases/virulencefinder.prepareref already exists. Cannot continue\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/homes/11025234/miniconda3/bin/ariba\", line 292, in <module>\n",
      "    args.func(args)\n",
      "  File \"/shared/homes/11025234/miniconda3/lib/python3.6/site-packages/ariba/tasks/prepareref.py\", line 31, in run\n",
      "    preparer.run(options.outdir)\n",
      "  File \"/shared/homes/11025234/miniconda3/lib/python3.6/site-packages/ariba/ref_preparer.py\", line 146, in run\n",
      "    raise Error('Error! Output directory ' + outdir + ' already exists. Cannot continue')\n",
      "ariba.ref_preparer.Error: Error! Output directory /shared/homes/11025234/ARIBA_databases/EC_customDB.prepareref already exists. Cannot continue\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/homes/11025234/miniconda3/bin/ariba\", line 292, in <module>\n",
      "    args.func(args)\n",
      "  File \"/shared/homes/11025234/miniconda3/lib/python3.6/site-packages/ariba/tasks/prepareref.py\", line 31, in run\n",
      "    preparer.run(options.outdir)\n",
      "  File \"/shared/homes/11025234/miniconda3/lib/python3.6/site-packages/ariba/ref_preparer.py\", line 146, in run\n",
      "    raise Error('Error! Output directory ' + outdir + ' already exists. Cannot continue')\n",
      "ariba.ref_preparer.Error: Error! Output directory /shared/homes/11025234/ARIBA_databases/E_coli_phylogroup.prepareref already exists. Cannot continue\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/homes/11025234/miniconda3/bin/ariba\", line 292, in <module>\n",
      "    args.func(args)\n",
      "  File \"/shared/homes/11025234/miniconda3/lib/python3.6/site-packages/ariba/tasks/prepareref.py\", line 31, in run\n",
      "    preparer.run(options.outdir)\n",
      "  File \"/shared/homes/11025234/miniconda3/lib/python3.6/site-packages/ariba/ref_preparer.py\", line 146, in run\n",
      "    raise Error('Error! Output directory ' + outdir + ' already exists. Cannot continue')\n",
      "ariba.ref_preparer.Error: Error! Output directory /shared/homes/11025234/ARIBA_databases/EcOH.prepareref already exists. Cannot continue\n",
      "mv: cannot move `/shared/homes/11025234/ARIBA_databases/database_warnings' to a subdirectory of itself, `/shared/homes/11025234/ARIBA_databases/database_warnings/database_warnings'\n"
     ]
    }
   ],
   "source": [
    "!ariba prepareref -f ~/ARIBA_databases/plasmidfinder.fa -m ~/ARIBA_databases/plasmidfinder.tsv ~/ARIBA_databases/plasmidfinder.prepareref > ~/ARIBA_databases/plasmidfinder.warnings\n",
    "!ariba prepareref -f ~/ARIBA_databases/resfinder.fa -m ~/ARIBA_databases/resfinder.tsv ~/ARIBA_databases/resfinder.prepareref > ~/ARIBA_databases/resfinder.warnings\n",
    "!ariba prepareref -f ~/ARIBA_databases/virulencefinder.fa -m ~/ARIBA_databases/virulencefinder.tsv ~/ARIBA_databases/virulencefinder.prepareref  > ~/ARIBA_databases/virulencefinder.warnings\n",
    "# No prepareref step required for MLST database\n",
    "!ariba prepareref --all_coding no -f ~/ARIBA_databases/EC_customDB.fasta ~/ARIBA_databases/EC_customDB.prepareref > ~/ARIBA_databases/EC_customDB.warnings\n",
    "!ariba prepareref --all_coding no -f ~/ARIBA_databases/E_coli_phylogroup.fasta ~/ARIBA_databases/E_coli_phylogroup.prepareref > ~/ARIBA_databases/E_coli_phylogroup.warnings\n",
    "!ariba prepareref --all_coding no -f ~/ARIBA_databases/O_H_types.fsa ~/ARIBA_databases/serofinder_EcOH.prepareref > ~/ARIBA_databases/serofinder_EcOH.warnings\n",
    "!mkdir ~/ARIBA_databases/database_warnings; mv ~/ARIBA_databases/*.warnings ~/ARIBA_databases/database_warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CURRENTLY NOT WORKING - Submitting ARIBA jobs to the cluster\n",
    "Jobs were submitted with qsub to a high powered computing cluster at UTS. The qsub script can be accessed on github as below. If you would like to use it on a different cluster this script will require some editing.\n",
    "\n",
    "Syntax for this qsub job submission is as follows:\n",
    "\n",
    "``` qsub -v READ_PATH=\\*path_to_reads\\*,REF=\\*prepareref_database\\*,OUT=\\*output_suffix\\*,EMAIL=\\*user_email\\* ~/qsub/ariba_scratch.qsub ```\n",
    "\n",
    "Currently, a student number is required. Providing this will give alerts from the HPC via student email as to when a job has started, has been abored, or has completed. Changing \"$STU_NO\" on line 9 of ariba_scratch.qsub to any valid email address will also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-19 09:52:18--  https://raw.githubusercontent.com/maxlcummins/Bioinformatics/master/ariba_scratch.qsub\n",
      "Resolving raw.githubusercontent.com... 151.101.28.133\n",
      "Connecting to raw.githubusercontent.com|151.101.28.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1409 (1.4K) [text/plain]\n",
      "Saving to: `/shared/homes/11025234/qsub/ariba_scratch.qsub'\n",
      "\n",
      "100%[======================================>] 1,409       --.-K/s   in 0s      \n",
      "\n",
      "2018-11-19 09:52:18 (336 MB/s) - `/shared/homes/11025234/qsub/ariba_scratch.qsub' saved [1409/1409]\n",
      "\n",
      "98362.hpcnode0\n",
      "98363.hpcnode0\n",
      "98364.hpcnode0\n",
      "98365.hpcnode0\n",
      "98366.hpcnode0\n",
      "98367.hpcnode0\n",
      "98368.hpcnode0\n"
     ]
    }
   ],
   "source": [
    "#Note that before we submit the jobs we first download the ariba_scratch.qsub file from github with 'wget'\n",
    "!rm ~/qsub/ariba_scratch.qsub\n",
    "!wget https://raw.githubusercontent.com/maxlcummins/Bioinformatics/master/ariba_scratch.qsub -P ~/qsub\n",
    "\n",
    "#Submit ariba run via qsub to the computing cluster\n",
    "!cd ~\n",
    "!qsub -v READ_PATH=BigChook_Reads/reads,REF=ARIBA_databases/plasmidfinder.prepareref,OUT=plasmidfinder,STU_NO=11025234 ~/qsub/ariba_scratch.qsub\n",
    "!qsub -v READ_PATH=BigChook_Reads/reads,REF=ARIBA_databases/resfinder.prepareref,OUT=resfinder,STU_NO=11025234 ~/qsub/ariba_scratch.qsub\n",
    "!qsub -v READ_PATH=BigChook_Reads/reads,REF=ARIBA_databases/virulencefinder.prepareref,OUT=virulencefinder,STU_NO=11025234 ~/qsub/ariba_scratch.qsub\n",
    "!qsub -v READ_PATH=BigChook_Reads/reads,REF=ARIBA_databases/get_mlst/ref_db,OUT=MLST,STU_NO=11025234 ~/qsub/ariba_scratch.qsub\n",
    "!qsub -v READ_PATH=BigChook_Reads/reads,REF=ARIBA_databases/EC_customDB.prepareref,OUT=EC_customDB,STU_NO=11025234 ~/qsub/ariba_scratch.qsub\n",
    "!qsub -v READ_PATH=BigChook_Reads/reads,REF=ARIBA_databases/E_coli_phylogroup.prepareref,OUT=E_coli_phylogroup,STU_NO=11025234 ~/qsub/ariba_scratch.qsub\n",
    "!qsub -v READ_PATH=BigChook_Reads/reads,REF=ARIBA_databases/serofinder_EcOH.prepareref,OUT=serofinder_EcOH,STU_NO=11025234 ~/qsub/ariba_scratch.qsub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarising output from ARIBA runs\n",
    "Next we need to summarise the output for each respective run. For more detailed information about this check out the GitHub repo on [ARIBAlord](https://github.com/maxlcummins/ARIBAlord).\n",
    "\n",
    "Say we have 100 read-pairs, corresponding to 100 samples, all beginning with 'AVC'. Following the completion of our job submissions in the step above (you should be notified by email), all read-pairs will be associated with an output folder for each database we used to screen the samples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `/shared/homes/11025234/BigChook_Reads/reads/BigChook3_ARIBA_summaries': File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ariba summary --no_tree --cluster_cols assembled,ref_seq ~/BigChook_Reads/reads/BigChook3_plasmidfinder ~/BigChook_Reads/reads/*plasmidfinder.out/report.tsv\n",
    "ariba summary --no_tree --cluster_cols assembled,ref_seq ~/BigChook_Reads/reads/BigChook3_resfinder ~/BigChook_Reads/reads/*resfinder.out/report.tsv\n",
    "ariba summary --no_tree --cluster_cols assembled,ref_seq ~/BigChook_Reads/reads/BigChook3_virulencefinder ~/BigChook_Reads/reads/*virulencefinder.out/report.tsv\n",
    "#MLST must be processed a bit differently\n",
    "ariba summary --no_tree --cluster_cols assembled,ref_seq ~/BigChook_Reads/reads/BigChook3_EC_customDB ~/BigChook_Reads/reads/*EC_customDB.out/report.tsv\n",
    "ariba summary --no_tree --cluster_cols assembled,ref_seq ~/BigChook_Reads/reads/BigChook3_E_coli_phylogroup ~/BigChook_Reads/reads/*E_coli_phylogroup.out/report.tsv\n",
    "ariba summary --no_tree --cluster_cols assembled,ref_seq ~/BigChook_Reads/reads/BigChook3_serofinder_EcOH ~/BigChook_Reads/reads/*serofinder_EcOH.out/report.tsv\n",
    "\n",
    "#MLST pre-processing\n",
    "for f in ~/BigChook_Reads/reads/*MLST.out/mlst_report.tsv; do paste $f <(yes $f | head -n $(cat $f | wc -l)) > $f.new; done\n",
    "cat ~/BigChook_Reads/reads/*MLST.out/*.new > ~/BigChook_Reads/reads/BigChook3_MLST.tsv\n",
    "\n",
    "mkdir ~/BigChook_Reads/reads/BigChook3_ARIBA_summaries\n",
    "cp ~/BigChook_Reads/reads/*.csv ~/BigChook_Reads/reads/BigChook3_MLST.tsv ~/BigChook_Reads/reads/BigChook3_ARIBA_summaries\n",
    "rm ~/BigChook_Reads/reads/BigChook3_ARIBA_summaries/*.phandango*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIBAlord - combining ARIBA outputs\n",
    "Here we create a new conda environment called ARIBAlord for running... ARIBAlord...\n",
    "\n",
    "We install the required dependencies, activate the environment and clone the github to the home directory. We then run the python executable ```ARIBAlord.py``` from within the downloaded github repo and feed in two variables:\n",
    "1. The path to the ARIBA_summaries (contains the outputs of our ariba summary and MLST processing steps)\n",
    "2. An output prefix (in this case I have chosen BigChook3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ARIBAlord' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!source deactivate\n",
    "!source activate aribalord\n",
    "\n",
    "!cd ~\n",
    "!git clone https://github.com/maxlcummins/ARIBAlord.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to clean sample names in non MLST ariba outputs\n",
    "!perl -pi -e 's/_R1.*tsv//g' ~/BigChook_Reads/reads/BigChook3_ARIBA_summaries/BigChook3_*.csv\n",
    "!perl -pi -e 's/.*AVC/AVC/g' ~/BigChook_Reads/reads/BigChook3_ARIBA_summaries/BigChook3_*.csv\n",
    "\n",
    "#to clean sample names in MLST ariba output\n",
    "!perl -pi -e 's/_R1.*tsv//g' ~/BigChook_Reads/reads/BigChook3_ARIBA_summaries/BigChook3_MLST.tsv\n",
    "!perl -pi -e 's/\\/shared.*reads\\///g' ~/BigChook_Reads/reads/BigChook3_ARIBA_summaries/BigChook3_MLST.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running geno on CSV files in /shared/homes/11025234/BigChook_Reads/reads/BigChook3_ARIBA_summaries... \n",
      "Argument '--trim' not used.\n",
      "\t Found csv file 1: /shared/homes/11025234/BigChook_Reads/reads/BigChook3_ARIBA_summaries/BigChook3_resfinder.csv...\n",
      "\t Found csv file 2: /shared/homes/11025234/BigChook_Reads/reads/BigChook3_ARIBA_summaries/BigChook3_plasmidfinder.csv...\n",
      "\t Found csv file 3: /shared/homes/11025234/BigChook_Reads/reads/BigChook3_ARIBA_summaries/BigChook3_virulencefinder.csv...\n",
      "\t Found csv file 4: /shared/homes/11025234/BigChook_Reads/reads/BigChook3_ARIBA_summaries/BigChook3_EcOH.csv...\n",
      "\t Found csv file 5: /shared/homes/11025234/BigChook_Reads/reads/BigChook3_ARIBA_summaries/BigChook3_E_coli_phylogroup.csv...\n",
      "\t Found csv file 6: /shared/homes/11025234/BigChook_Reads/reads/BigChook3_ARIBA_summaries/BigChook3_EC_customDB.csv...\n",
      "\n",
      "Cleaning headers of processed CSV files, adding identifiers prefixes\n",
      "\n",
      " srst2 serotype detected\n",
      "\n",
      "\n",
      "Generating MLST table:\n",
      "\tFound MLST file: /shared/homes/11025234/BigChook_Reads/reads/BigChook3_ARIBA_summaries/BigChook3_MLST.tsv...\n",
      "       ST MLST_adk MLST_fumC   ...   MLST_purA MLST_recA    name\n",
      "0      95       37        38   ...          11        26  AVC103\n",
      "1      ST      adk      fumC   ...        purA      recA  AVC105\n",
      "2     973      154       187   ...         129         4  AVC105\n",
      "3      ST      adk      fumC   ...        purA      recA  AVC106\n",
      "4    8398       35       132   ...           5         4  AVC106\n",
      "5      ST      adk      fumC   ...        purA      recA  AVC107\n",
      "6    1163       20        45   ...          50        46  AVC107\n",
      "7      ST      adk      fumC   ...        purA      recA  AVC109\n",
      "8    1303       10         7   ...          35         2  AVC109\n",
      "9      ST      adk      fumC   ...        purA      recA   AVC10\n",
      "10    429       97        40   ...          28        66   AVC10\n",
      "11     ST      adk      fumC   ...        purA      recA  AVC111\n",
      "12    354       85        88   ...          58        62  AVC111\n",
      "13     ST      adk      fumC   ...        purA      recA  AVC113\n",
      "14    155        6         4   ...           8        14  AVC113\n",
      "15     ST      adk      fumC   ...        purA      recA   AVC11\n",
      "16    429       97        40   ...          28        66   AVC11\n",
      "17     ST      adk      fumC   ...        purA      recA  AVC123\n",
      "18     10       10        11   ...           8         2  AVC123\n",
      "19     ST      adk      fumC   ...        purA      recA  AVC126\n",
      "20    350        6        31   ...           1         2  AVC126\n",
      "21     ST      adk      fumC   ...        purA      recA   AVC12\n",
      "22    350        6        31   ...           1         2   AVC12\n",
      "23     ST      adk      fumC   ...        purA      recA  AVC148\n",
      "24    350        6        31   ...           1         2  AVC148\n",
      "25     ST      adk      fumC   ...        purA      recA  AVC149\n",
      "26    350        6        31   ...           1         2  AVC149\n",
      "27     ST      adk      fumC   ...        purA      recA  AVC153\n",
      "28     93        6        11   ...           8         6  AVC153\n",
      "29     ST      adk      fumC   ...        purA      recA  AVC154\n",
      "..    ...      ...       ...   ...         ...       ...     ...\n",
      "165    ST      adk      fumC   ...        purA      recA    AVC5\n",
      "166   117       20        45   ...          32         2    AVC5\n",
      "167    ST      adk      fumC   ...        purA      recA   AVC60\n",
      "168   117       20        45   ...          32         2   AVC60\n",
      "169    ST      adk      fumC   ...        purA      recA   AVC61\n",
      "170   429       97        40   ...          28        66   AVC61\n",
      "171    ST      adk      fumC   ...        purA      recA   AVC64\n",
      "172    57        6        31   ...           1         2   AVC64\n",
      "173    ST      adk      fumC   ...        purA      recA   AVC65\n",
      "174   973      154       187   ...         129         4   AVC65\n",
      "175    ST      adk      fumC   ...        purA      recA   AVC77\n",
      "176   117       20        45   ...          32         2   AVC77\n",
      "177    ST      adk      fumC   ...        purA      recA   AVC79\n",
      "178   117       20        45   ...          32         2   AVC79\n",
      "179    ST      adk      fumC   ...        purA      recA    AVC7\n",
      "180  3998        6        29   ...           8         7    AVC7\n",
      "181    ST      adk      fumC   ...        purA      recA   AVC80\n",
      "182   117       20        45   ...          32         2   AVC80\n",
      "183    ST      adk      fumC   ...        purA      recA    AVC8\n",
      "184   429       97        40   ...          28        66    AVC8\n",
      "185    ST      adk      fumC   ...        purA      recA   AVC91\n",
      "186   973      154       187   ...         129         4   AVC91\n",
      "187    ST      adk      fumC   ...        purA      recA   AVC94\n",
      "188  1125        6         4   ...          26         7   AVC94\n",
      "189    ST      adk      fumC   ...        purA      recA   AVC96\n",
      "190   117       20        45   ...          32         2   AVC96\n",
      "191    ST      adk      fumC   ...        purA      recA   AVC97\n",
      "192    79       37        38   ...           7        26   AVC97\n",
      "193    ST      adk      fumC   ...        purA      recA   AVC98\n",
      "194  1640        6        31   ...           1         2   AVC98\n",
      "\n",
      "[195 rows x 9 columns]\n",
      "\n",
      "EcOH detected\n",
      "\n",
      "Phylogroup detected\n",
      "arpA detected\n",
      "tspE4.C2 detected\n",
      "yjaA detected\n",
      "chuA detected\n",
      "\n",
      "Writing simplified ARIBA table to /shared/homes/11025234/BigChook_Reads/reads/BigChook3_simple.csv\n",
      "\n",
      "Writing full ARIBA table to /shared/homes/11025234/BigChook_Reads/reads/BigChook3_full.csv\n",
      "\n",
      "\n",
      "Writing full serotype table /shared/homes/11025234/BigChook_Reads/reads/BigChook3_sero.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ~/ARIBAlord/ARIBAlord ~/BigChook_Reads/reads/BigChook3_ARIBA_summaries ~/BigChook_Reads/reads/BigChook3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
